{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>template: home.html title: Material for MkDocs</p>"},{"location":"ML/intro/","title":"Intro","text":"<p>There is data available. And there exists a pattern in the data which can't be pinned down mathematically. There when we use machine learning. </p> <p>In case of machine learning:  </p> <p>Types of machine learning:  - Supervised : Labels / ground truths are present - Unsupervised: Labels / ground truths are absent  - Reinforcement: The machine learns from mistakes like us hoomans </p> <p>Typical machine learning problems fall under one of the following category:  </p>"},{"location":"ML/kmeans/","title":"Kmeans Algorithm","text":"<p>Steps: - Given a dataset of size <code>N</code>, randomly initialize <code>k</code> centroids if you feel the dataset can be clustered into <code>k</code> different clusters</p> <p>\\(C = \\Set{c_1,..,c_K}\\)  | Set of <code>K</code> Centroids </p> <p>\\(S = \\Set{s_1,..,s_K}\\)  | Set of <code>K</code> Clusters </p> <p>Intra-cluster variance is the objective function which has to be minimized in k-means algorithm:     </p> <ul> <li> <p>Class assignment</p> <p></p> </li> <li> <p>Update centroids</p> <p></p> </li> <li> <p>Repeat the process until the cluster centroids change any further, i.e. repeat until convergence </p> </li> </ul> <p>Complexity in one iteration:  - k n t   k = K clustures | n = No of samples | t = time taken</p> <p>It is sensitive to the randomly chosen centroids </p>"},{"location":"ML/kmeans/#clustering-performance","title":"Clustering performance","text":""},{"location":"ML/kmeans/#silhouette-coefficient","title":"Silhouette Coefficient","text":"\\[s=\\dfrac{b-a}{max(b,a)}\\] <p>a : the mean distance between a datapoint and the other points from the same cluster </p> <p>b : the mean distance between a datapoint and all the other points in the next nearest cluster. </p> <p>Example</p> <pre><code>from sklearn import metrics \n\nmetrics.silhouette_score(data,labels,metric='euclidean')\n# returns the silhouette score\n</code></pre>"},{"location":"ML/sgd/","title":"Gradient Descent","text":"<pre><code>from matplotlib.animation import FuncAnimation as animate\nimport torch \nfrom matplotlib import pyplot as plt \n\n\nfig, ax = plt.subplots()\nf = lambda x : (x-5)**2 \nx1 = torch.linspace(-10,20,100)\ny1 = f(x1)\nw = -5.0\nfdashx = lambda x: 2*(x-5)\n\ntangent = lambda x,w : fdashx(w) * (x-w) + f(w)\na = -8.5\n\nlr = 0.01\ndef sgd(i):\n    global w,a\n    ydash = fdashx(w)\n    w = w - lr*ydash\n    a = a - lr*ydash\n    y = f(w)\n    xs = torch.tensor([a,a+5])\n    ax.clear()\n    ax.plot(x1,y1)\n    ax.plot(xs.numpy(),tangent(xs,w).numpy())\n    ax.scatter(w,y,c='y')\n\nplots = animate(fig, sgd, frames=500, interval=10, repeat=False)\nplt.show()\n</code></pre> output <pre><code>fig, ax = plt.subplots()\nx1 = torch.linspace(-10,20,100)\ny1 = (x1-5)**2\n\nlearning_rate = 0.01\n\nx = torch.tensor([-5.0], requires_grad=True)\na=-8.5\noptimizer =  torch.optim.SGD([x],lr=learning_rate)\ndef gradsgd(_):\n    optimizer.zero_grad()\n    global x,a\n    y = lambda x: (x - 5) ** 2\n    y(x).backward()\n    optimizer.step()\n    # grad = x.grad\n    tangent_y = lambda a:x.grad*(a-x)+ y(x)\n\n    # with torch.no_grad():\n    #     x = x - learning_rate * x.grad\n    #     a-= learning_rate*grad.item()\n    # x.requires_grad = True\n\n    xx = torch.tensor([-10,20])\n    ax.clear()\n    ax.plot(x1,y1)\n    ax.plot(xx,tangent_y(xx).detach().numpy()) \n    ax.scatter(x.detach().numpy(),((x-5)**2).detach().numpy(),c='r')\n\n\nplots1 = animate(fig, gradsgd, frames=500, interval=10, repeat=False)\nplt.show()\n</code></pre> <pre><code>fig, ax = plt.subplots()\nx1 = torch.linspace(-10,20,100)\ny1 = (x1-5)**2\n\nlearning_rate = 0.01\n\nx = torch.tensor([-5.0], requires_grad=True)\n\ndef gradsgd(_):\n    global x\n    y = (x - 4) ** 2\n    y.backward()\n    with torch.no_grad():\n        x = x - learning_rate * x.grad\n\n    x.requires_grad = True\n    ax.clear()\n    ax.plot(x1,y1)\n    ax.scatter(x.detach().numpy(),((x-5)**2).detach().numpy(),c='r')\n\n\nplots1 = animate(fig, gradsgd, frames=500, interval=10, repeat=False)\nplt.show()\n</code></pre>"},{"location":"ML/sgd/#problem-with-gradient-descent","title":"Problem with gradient descent","text":"<ol> <li> <p>It diverges if $lr &gt; 1 $.</p> </li> <li> <p>It may stuck to a local minimum, instead of global minimum. </p> </li> <li> <p>It is difficult to interpret whether it is increasing or decreasing near a saddle point. It does overcome it but it might take a long time.</p> </li> <li> <p>For small gradient the learning becomes slow.</p> </li> <li> <p>The hyper parameter in this case, the learning rate has to be determined beforehand, which is often difficult to choose if dimension increases. </p> </li> </ol>"},{"location":"ML/sgd/#how-to-improve-gd","title":"How to improve GD ?","text":"<ol> <li> <p>Add a momentum term, in other words, Nesterov's Accelerated Gradient descent.</p> </li> <li> <p>AdaGrad</p> </li> <li> <p>Higher order derivate, like hessian : Newton's method, Quasi Newton's methods like BFGS</p> </li> <li> <p>Stochastic Gradient Descent. </p> </li> </ol>"},{"location":"data%20structures/arrays/","title":"Arrays in python","text":"<p>Python has dynamic arrays called <code>list</code>. </p> <p>Lists are basically arrays of points to an python object.  Everything in python are objects. And every object contains a <code>reference counter</code> - it keep tracks of all the reference to the object, <code>pointer to the type</code>, and finally the data to be stored inside that object. </p> <p>Due to these things an object takes up additional space of 16 bytes. </p> <p>1D array: <code>byte address of element i = base address + (size of each element)*(i - first_index)</code></p> <p>Example</p> <p>a = [0,1,2,3,4,5,6] | base address = 1000 | size of element = 2 byte | find location of a[4]</p> <p>location of <code>a[4]</code> = 1000 + 2*(4-0) = 1008</p> <p></p> <p>Insertion &amp; Deletion</p> <ul> <li>Best case : at the end,  \\(\\Omega(1)\\)</li> <li>Worst case: at the beginning, O(n)</li> <li>Average case: in the middle, \\(\\Theta(n)\\) </li> </ul>"},{"location":"data%20structures/ll/","title":"Linked lists","text":"<ul> <li>No fixed storage, i.e., dynamic memory allocation</li> <li><code>insertion</code>, <code>deletion</code> takes O(1)</li> <li>It is a linear data structure, which are not stored in a contiguous memory location.</li> </ul>"},{"location":"data%20structures/ll/#implementation-of-linked-list-in-python","title":"Implementation of Linked List in Python","text":"<p>Typical nodes in linked list looks like:  <pre><code>class Node:\n    def __init__(self,data:Any,\n                nextNode:Optional[Any]=None) -&gt; None:\n        self.data = data # to store the actual data\n        self.next = nextNode # pointer to next node\n</code></pre></p> <pre><code>class LinkedList(object):\n    def __init__(self,*args):\n        self.head = None\n        self.tail = None\n        self.__size = 0 \n\n    def append(self,data):...\n    def pop(self):...\n    def insert(self,at,data):...\n    def remove(self,index):...\n    def __len__(self):...\n    def __add__(self):... # I have defined this in such a way that\n    #  you can concatenate two list objects with a '+' operator \n    def reverse(self):...\n</code></pre> <p>click here to see the code. OR use this to install the addtional data structures  <pre><code>pip install Gsauce-pyds\n</code></pre></p> <p>Example</p> <pre><code>    from dstructure.linkedlist import LinkedList\n    l = LinkedList(range(8))\n    print(l)\n    # LinkedList([0, 1, 2, 3, 4, 5, 6, 7])\n\n    l.append('last')\n    print(l)\n    # LinkedList([0, 1, 2, 3, 4, 5, 6, 7, last])\n\n    print(len(l))\n    #9\n\n    print(l[0],l[5],l[8])\n    # Node(data = 0) Node(data = 5) Node(data = last)\n\n    print(l.head,l.tail)\n    # Node(data = 0) Node(data = last)\n\n    l[8]=8\n    print(l)\n    # LinkedList([0, 1, 2, 3, 4, 5, 6, 7, 8])\n\n    l.reverse()\n    print(l,'head: ',l.head,'tail: ',l.tail,sep='\\n')\n    # LinkedList([8, 7, 6, 5, 4, 3, 2, 1, 0])\n    # head: \n    # Node(data = 8)\n    # tail: \n    # Node(data = 0)\n\n    print(l[:3],l[3:])\n    # LinkedList([8, 7, 6]) LinkedList([5, 4, 3, 2, 1, 0])\n\n    l.insert(0,'zero')\n    l.insert(3,'three')\n    print(l,len(l),l.head,l.tail,sep='\\n')\n    # LinkedList([zero, 8, 7, three, 6, 5, 4, 3, 2, 1, 0])\n    # 11\n    # Node(data = zero)\n    # Node(data = 0)\n\n    l.remove(0)\n    l.remove(3)\n    print(l,l.head,l.tail,sep='\\n')\n    # LinkedList([8, 7, three, 5, 4, 3, 2, 1, 0])\n    # Node(data = 8)\n    # Node(data = 0)\n\n    a = LinkedList('a b c'.split())\n    print(a)\n    a + l  # concatenation will overwrite the original list, in this case it's a\n    print(a)\n    # LinkedList([a, b, c])\n    # LinkedList([a, b, c, 8, 7, three, 5, 4, 3, 2, 1, 0])\n\n\n    print(len(a))\n    a.pop()\n    a.pop()\n    print(len(a))\n    # 12\n    # 10\n</code></pre> <p>Note</p> <ul> <li>Binary search takes O(n)</li> <li>Concatenation takes O(m+n)</li> </ul>"},{"location":"data%20structures/ll/#doubly-linked-list","title":"Doubly linked list","text":"<p>Linked list but each node contains a pointer to next and previous node. </p> <p>Doubly linked list node <pre><code>class node:\n    def __init__(self,data,prev=None,next=None):\n        self.data = data \n        self.prev = prev\n        self.next = next\n</code></pre></p> <p>Doubly linked list class: <pre><code>class DoublyLinkedList(object):\n    def __init__(self,*e):\n        self.head = None # reference/pointer to the starting(head) node\n        self.tail = None # pointer to the end node\n        self.__size =0 # to track the length of the list\n\n    def __len__(self):... # returns the length of the list\n    def append(self,data):... # appends a node at the end of the list\n    def pop(self):... # removes an element at the end of the list\n    def reverse(self):... # reverses the list \n</code></pre></p> <p>Example</p> <pre><code>from dstructure.linkedlist import DoublyLinkedList\n\nl = DoublyLinkedList(range(4))\nprint(l)\n# DLList([0 1 2 3])\n\nl.append(4)\nprint(l,len(l))\n# DLList([0 1 2 3 4]) 5\n\nl.pop()\nprint(l)\n# DLList([0 1 2 3])\n\nprint(l.head,l[0],l.tail,l[3],sep='\\t')\n# Node(data = 0)    Node(data = 0)  Node(data = 3)  Node(data = 3)\n\nl.reverse()\nprint(l.head,l[0],l.tail,l[3],sep='\\t')\n# Node(data = 3)    Node(data = 3)  Node(data = 0)  Node(data = 0)\n\nprint(l.head.next,l.tail.prev)\n# Node(data = 2) Node(data = 1)\n\nprint(l)\nl[2]='two'\nprint(l)\n# DLList([3 2 1 0])\n# DLList([3 2 two 0])\n\nprint(l[:2],l[2:])\n# DLList([3 2]) DLList([two 0])\n</code></pre>"},{"location":"data%20structures/stack/","title":"Stack","text":"<ul> <li>An ordered list. </li> <li>Deletion and insertion all made at one end i.e., LIFO (last in first out)</li> <li>Stack has basic three operations which can be performed on stack, namely <code>push</code>, <code>pop</code>, and <code>peek</code> </li> <li><code>push</code>: to insert an element</li> <li><code>pop</code>: to remove the topmost element</li> <li><code>peek</code>: to look what's at the top of the stack without removing it</li> <li>Every stack operation takes constant time. O(1)</li> </ul> <p>Stacks are usually not dynamic in nature, if implemented using static arrays. But they can be made dynamic using linkedlists or dynamic lists in python. </p> <p>To see the implementation of <code>stack</code> using <code>linkedlist</code> click here.</p>"},{"location":"data%20structures/stack/#how-to-use-a-stack-in-python","title":"How to use a stack in python?","text":"<p>install the data structure library using  <pre><code>pip install Gsauce-pyds\n</code></pre></p> <p>Example</p> <p> <pre><code>from dstructure.stack import Stack2\n\ns = Stack2(range(3))\nprint(s,len(s))\n\n# output:\n# Stack([0,1,2]) 3\n\ns.push(3)\ns.push(5)\nprint(s,len(s))\n\n# output:\n# Stack([0,1,2,3,5]) 5\n\n\ns.pop()\nprint(s,len(s))\n\n# output:\n# Stack([0,1,2,3]) 4\n\nprint(s.peek)\n# 3\n</code></pre></p>"},{"location":"data%20structures/stack/#a-few-applications-of-stacks","title":"A few applications of stacks","text":"<ul> <li>Post fix notation </li> <li>Recursion </li> <li>Fibonacci series </li> <li>permutations ...... etc. </li> </ul>"},{"location":"python/complex/","title":"Complex Numbers in Python and intro to <code>class</code>","text":"<p>To define a complex number in python use <code>j</code>, equivalent to <code>i</code> in maths, which is nothing but our old friend <code>iota</code> square root of negative one. </p> <p>\\(j = \\sqrt{-1}\\) \\(z = a+bj\\)</p> <pre><code>z = 5+2j\nprint(z,type(z))\nprint(z.real,z.imag)\nprint('conjugate: ',z.conjugate())\n</code></pre> <p>Output</p> <pre><code>(5+2j) &lt;class 'complex'&gt;`\n5.0 2.0\nconjugate:  (5-2j)\n</code></pre>"},{"location":"python/complex/#complex-numbers-arithmetic","title":"Complex numbers arithmetic","text":"<pre><code>z = 5+2j\nw = 5+5j\nprint('Addition: ',z+w)\nprint('Substraction: ',z-w)\nprint('Multiplication: ',z*w)\nprint('Division: ',z/w)\n</code></pre> <p>Output</p> <p>Addition:  (10+7j)</p> <p>Substraction:  -3j</p> <p>Multiplication:  (15+35j)</p> <p>Division:  (0.7-0.3j)</p>"},{"location":"python/q/","title":"Decorators","text":"<p>In python everything is an object. Even functions, and they can be passed as parameters.</p>"},{"location":"python/q/#decorators-without-args-kwargs","title":"Decorators without args, *kwargs","text":"<pre><code>def outfun(somefun):\n    def infun():\n        print('inside infun')\n        return somefun()\n    return infun\n\ndef arbitraryFun():\n    print('this is an arbitrary function')\n\nfunc = outfun(arbitraryFun)\nfunc()\n</code></pre> output <p>inside infun</p> <p>this is an arbitrary function</p> <p>Note</p> <p>The above and the below codes does the same job, the only different is in syntax. In the code below there is an <code>@</code> symbol along with the function name. That is a decorator. </p> <pre><code>def outfun(somefun):\n    def infun():\n        print('inside infun')\n        return somefun()\n    return infun\n\n@outfun\ndef arbitraryFun():\n    print('this is an arbitrary function')\n\n# func = outfun(arbitraryFun)\n# func()\narbitraryFun()\n'''\nSo basically the commented lines and the last line are equivalent. \n'''\n</code></pre> output <p>inside infun</p> <p>this is an arbitrary function</p> <pre><code>def call_as_variable(fun):\n    return fun()\n\n@call_as_variable\ndef function():\n    s = 'you make me un poco loco'\n    return s \n\nprint(function)\n</code></pre> output <p>'you make me un poco loco'</p>"},{"location":"python/q/#decorators-with-arguments","title":"Decorators with arguments","text":"<pre><code>def outermost(arg):\n    def middle(func):\n        def inner():\n            func()\n            print(arg)\n            return  \n        return inner \n    return middle\n\n@outermost('thing')\ndef function():\n    print('some')\n\nfunction()\n</code></pre> output <p>some thing</p>"},{"location":"python/q/#class-decorators","title":"Class decorators","text":"<pre><code>class deco:\n    def __init__(self,func) -&gt; None:\n        self.func = func \n\n    def __call__(self,*a,**kw):\n        return self.func(*a,**kw) + ' with other string'\n\n@deco\ndef function():\n    return 'some string' \n\nfunction()\n</code></pre> output <p>'some string with other string'</p>"},{"location":"python/strings/","title":"Strings in python","text":""},{"location":"python/strings/#how-to-define-a-string-in-python","title":"How to define a string in python?","text":"<p>There are 4 different ways of doing that, you can use <code>single quotes</code>, <code>double quotes</code>, <code>triple quotes</code>, and <code>triple-double quotes</code>!!! Example:</p> <pre><code>s = \"Traditional way of defining a string\"\ns1 = 'Easier way of defining the string'\ns2 = '''This is another\nway of defining a string, in paragraphs'''\ns3 = \"\"\"\n\"roses are red, \nviolets are blue,\nif you like forking, \nthen you will like Github too.\"\n - this entire lame poem is also a string, quite long, but a string!\n\"\"\"\nprint(s,s1,s2,s3,sep='\\n')\n</code></pre> <p>Output</p> <pre><code>Traditional way of defining a string\nEasier way of defining the string\nThis is another\nway of defining a string, in paragraphs\n\n\"roses are red, \nviolets are blue,\nif you like forking, \nthen you will like Github too.\"\n- this entire lame poem is also a string, quite long, but a string!\n</code></pre>"},{"location":"python/strings/#string-indexing-and-slicing","title":"String Indexing and Slicing","text":"<p> You can access the characters of the string using <code>+ve</code> or <code>-ve</code> indices.</p> <p>Example: <pre><code>s = 'chocolate'\nprint(f'First character: {s[0]}') # this is f-sting**\nprint(f'Last character: {s[-1]}') \n# s[start index: end index :steps]\n#  Using this you can slice a part of a string\nprint(f'First 3 character: {s[:3]}') \nprint(f'Last 3 character: {s[-3:]}') \n</code></pre> <code>s[:3]</code> is same as <code>s[0:3]</code> so you needn't write the starting index.</p> <p>Output</p> <p>First character: c</p> <p>Last character: e</p> <p>First 3 character: cho</p> <p>Last 3 character: ate</p> <p>Now if you want to reverse the string, you can do that using slicing.  Just set steps to <code>-1</code> <pre><code>print(f'Reverse String is : {s[::-1]}') \n</code></pre></p> <p>Output</p> <p>Reverse String is : etalocohc</p> <p>If you want to take out the middle portion of chocolate then use this: <pre><code>s = 'chocolate'\nprint(f'Middle Portion : {s[3:7]}')\n</code></pre></p> <p>Output</p> <p>Middle Portion : cola</p> <p>And that's how you extract cola  from chocolate </p>"},{"location":"python/strings/#string-concatenation","title":"String concatenation","text":"<p>Want to concatenate multiple strings? LOL! Piece of . Just use the <code>+</code> operator. Like This: <pre><code>s = \"first Name\" + \"middle name\" + 'Last name'\nprint(s)\n</code></pre></p> <p>Output</p> <p>first Nameiddle nameLast name</p> <p>To concatenate same string multiple times use <code>*</code>:  <pre><code>s = 'LOL'*5\nprint(s)\n</code></pre></p> <p>Output</p> <p>LOLLOLLOLLOLLOL</p>"},{"location":"python/strings/#manipulate-your-strings","title":"Manipulate your strings:","text":""},{"location":"python/strings/#replace","title":".replace()","text":"<p>Consider this example: <pre><code>s = \"Angela is my girlfriend\"\n</code></pre> It's been ages with her, she has taken me for granted so I want to replace her. I don't know about real life, but this is easy in python. <pre><code>s = s.replace('Angela','Anjali') \n# .replace('old substring','new substring')\nprint(s)\n</code></pre></p> <p>Output</p> <p>Anjali is my girlfriend</p>"},{"location":"python/strings/#count","title":".count()","text":"<p>Returns the frequency of a substring</p> <pre><code>s =\"mi go, mi eat, amigo come, mi again go\"\nprint('Considering the whole string',s.count('mi'),sep='\\t')\nprint(f'Considering only \"{s[13:25]}\"',s.count('mi',13,25),sep='\\t')\n</code></pre> <p>Output</p> <p>Considering the whole string    4</p> <p>Considering only \", amigo come\" 1</p>"},{"location":"python/strings/#lower-upper-swapcase-capitalize","title":".lower(), .upper(), .swapcase() &amp; .capitalize()","text":"<pre><code>s =\"meRRy XmAs\"\nprint(s,s.lower(),s.upper(),s.capitalize(),s.swapcase(),s.title(),sep='\\n')\n</code></pre> <p>Output</p> <p>meRRy XmAs   | <code>original string</code> </p> <p>merry xmas  | <code>output of lower()</code> | <code>all letters -&gt; lowercase</code></p> <p>MERRY XMAS | <code>output of upper()</code> | <code>all letters -&gt; uppercase</code></p> <p>Merry xmas | <code>output of captalize()</code> | <code>only capitalizes the initials of the first word of a string</code></p> <p>MErrY xMaS | <code>output of swapcase()</code> | <code>self-explanatory</code></p> <p>Merry Xmas | <code>output of title()</code> | <code>capitalizes all the initials</code></p>"},{"location":"python/strings/#islower-isupper-isdigit","title":".islower(), .isupper(), .isdigit()","text":"<pre><code>print(\"UPPER\".isupper(),'uPPer'.isupper())\n        # True          False\n\nprint('loWer'.islower(),'lower'.islower())\n        # False             True\n\nprint('lol'.isdigit(),'92'.isdigit(),'3.4'.isdigit(),'/*-'.isdigit())\n#       False               True            False       False\n</code></pre>"},{"location":"python/strings/#isspace-istitle-isdecimal","title":".isspace(), .istitle(), .isdecimal()","text":"<pre><code>print('   '.isspace(),'    s   '.isspace())\n        # True               False\n\nprint('1.2'.isdecimal(),'12'.isdecimal(),'.12'.isdecimal())\n        # False          True                False\n\nprint('Title Shitel'.istitle(),'title shitel'.istitle(),'titlE'.istitle())\n#        True                    False                   False\n</code></pre>"},{"location":"python/strings/#isnumeric-isalpha-isalnum","title":".isnumeric(), .isalpha(), .isalnum()","text":"<pre><code>print('sOmethIng'.isalpha(),'anglepriya420'.isalpha())\n                # True           False\n\nprint('12'.isnumeric(),'1.2'.isnumeric(),'numeric'.isnumeric())\n            # True               False       False\n\nprint('aasd233'.isalnum(),'123'.isalnum(),'assert'.isalnum(),'1s#'.isalnum(),'spa ce'.isalnum())\n        # True              True          True                     False       False\n</code></pre>"},{"location":"python/strings/#endswith-startswith","title":".endswith(), .startswith()","text":"<pre><code>filename = 'unnecessary.csv'\nprint(filename.endswith('.csv'),filename.endswith('.CSV'))\n</code></pre> <p>Output</p> <p>True          False</p> <pre><code>print('Miss Hitler'.startswith('Miss'))\n            # True\nprint('Miss Hitler'.startswith('miss'))\n            # False\nprint('Miss Hitler'.lower().startswith('miss'))\n            # True\n</code></pre>"},{"location":"python/strings/#split-join","title":".split(), .join()","text":"<p><code>split()</code> method splits the string on the basis of a delimiter, provided by the user. returns a list of words.</p> <p>By default the delemiter is set to <code>' '</code>.</p> <p>Example 1 <pre><code>s = 'This is a string'\nprint(s)\ns = s.split()\nprint(s)\n</code></pre></p> <p>Output</p> <p>This is a string</p> <p>['This', 'is', 'a', 'string']</p> <p>Example 2 <pre><code>s = 'name,age,sex,roll,num'\nprint(s)\ns = s.split(',')\nprint(s)\n</code></pre></p> <p>Output</p> <p>name,age,sex,roll,num</p> <p>['name', 'age', 'sex', 'roll', 'num']</p> <p><code>.join()</code> method concatenate all strings of a given iterable( list, tuples,.etc)</p> <p>Example:  <pre><code>l = ['name', 'age', 'sex', 'roll', 'num']\nprint(' '.join(l))\n</code></pre></p> <p>Output</p> <p>name age sex roll num</p>"},{"location":"python/strings/#regex","title":"REGEX","text":"<p>Used to search for a specific pattern in a text. </p> <p><code>[]{}()\\^$|?*+</code> are few characters which we need to expace using a <code>\\</code> as a prefix before search for these. </p> <p><code>.</code> matches all characters except newline</p> <p><code>\\d</code> matches all the digits whereas <code>\\D</code> matches everthing but digits. </p> <p><code>\\w</code> = word | <code>\\W</code> = Not a word</p> <p><code>\\s</code> = space | <code>\\S</code> = not a space</p> <p><code>\\b</code> = word boundary | <code>\\B</code> = Not a word boundary</p> <p><code>^</code> = Start of a string</p> <p><code>$</code> = end of a string</p> <p><code>*</code> = 0 or more</p> <p><code>+</code> = 1 or more </p> <p><code>?</code> = 0 or one match </p> <p><code>{5}</code> = exact number of match </p> <p><code>{min,max}</code> = range of numbers </p>"},{"location":"pytorch/grad/","title":"Autograd","text":""},{"location":"pytorch/grad/#autograd","title":"Autograd","text":"<p><pre><code>import torch \n\nx = torch.tensor([1,5,2],dtype=torch.float32,requires_grad=True)\nprint(x)\n# tensor([1., 5., 2.], requires_grad=True)\n</code></pre> Consider the following function : $ y = x^{2} + 2x +1 $</p> <pre><code>a =  x**2 \nb = a+ 2*x \ny = (b +1).sum() \nprint(y)\ny.backward()\nprint(x.grad)\n</code></pre> output <p>tensor(49., grad_fn=) <p>tensor([ 4., 12.,  6.])</p> Hidden <pre><code>import torch \nfrom torch import optim \nfrom matplotlib import pyplot as plt \n\ndef derivative_viz(plot:bool=False):\n    def middle(fun):\n        def inner(*a,**k):\n            start = k.get('start',-13)\n            end = k.get('end',17)\n            spacing = k.get('steps',100)\n            if plot:\n                x = torch.linspace(start,end,spacing,requires_grad=True)\n                print(f'Graph of ({fun.__name__})')\n                y = fun(x)\n                plt.plot(x.detach().numpy(),y.detach().numpy())\n                plt.show()\n                print(f'Graph of d/dx({fun.__name__})')\n                y = y.sum()\n                y.backward()\n                plt.plot(x.detach().numpy(),x.grad.detach().numpy())\n            return fun(*a)\n        return inner \n    return middle\n\n@derivative_viz()\ndef sin(x):\n    return torch.sin(x)\n\nxa = torch.tensor([45])\nprint(sin(xa,start=0,stop=20))\n</code></pre>"},{"location":"pytorch/tensor/","title":"PyTorch tensors","text":"<pre><code>import torch \nimport numpy as np \n</code></pre>"},{"location":"pytorch/tensor/#convert-a-list-to-a-tensor","title":"Convert a list to a tensor","text":"<pre><code>x = torch.tensor([1,2,3,6])\nprint(x,x.dtype,type(x))\n</code></pre> Output <p><code>tensor([1, 2, 3, 6]) torch.int64 &lt;class 'torch.Tensor'&gt;</code></p>"},{"location":"pytorch/tensor/#torchones_like","title":"torch.ones_like()","text":"<pre><code>x1 = torch.ones_like(x) # returns a tensor of same \n# shape as the input tensor but filled with ones hence the name \"ones like\"\nprint(x1,x,f'shape {x1.shape}',sep='\\n')\n</code></pre> Output <pre><code>tensor([1, 1, 1, 1])\ntensor([1, 2, 3, 6])\nshape torch.Size([4])\n</code></pre>"},{"location":"pytorch/tensor/#torchrand_like","title":"torch.rand_like()","text":"<pre><code>x2 = torch.rand_like(x1,dtype= torch.float16) # silimar to ones_like but fills the \n# tensor with random numbers\nx2_ = torch.rand_like(x,dtype=torch.float16)\nprint(x2,x2_,sep='\\n')\n</code></pre> Output <pre><code>tensor([0.2275, 0.6938, 0.1953, 0.4771], dtype=torch.float16)\ntensor([0.1182, 0.9717, 0.3511, 0.4023], dtype=torch.float16)\n</code></pre>"},{"location":"pytorch/tensor/#torchtensornumpy-torchfrom_numpy","title":"torch.tensor.numpy() &amp; torch.from_numpy()","text":"<p>convert any tensor to numpy ndarray</p> <pre><code>x1 = x1.numpy()\nprint(type(x1),x1)\nprint(torch.from_numpy(np.zeros(10)))\n</code></pre> Output <pre><code>&lt;class 'numpy.ndarray'&gt; [1 1 1 1]\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)\n</code></pre>"},{"location":"pytorch/tensor/#torchrand-torchones-torchzeros","title":"torch.rand(), torch.ones(), torch.zeros()","text":"<pre><code>print('rand(): ', torch.rand((1,2)))\nprint('ones(): ', torch.ones((2,3)))\nprint('zeros(): ', torch.zeros((3,3)))\n# torch.these_functions(shape)\n</code></pre> Output <pre><code>rand():  tensor([[0.4445, 0.0895]])\nones():  tensor([[1., 1., 1.],\n        [1., 1., 1.]])\nzeros():  tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n</code></pre>"},{"location":"pytorch/tensor/#move-tensors-between-platforms","title":"move tensors between platforms","text":"<pre><code>platform = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'device is {platform}')\nx = x.to(platform)\nprint(x.device,'\\t' ,torch.cuda.get_device_name())\n</code></pre> Output <pre><code>device is cuda\ncuda:0   NVIDIA GeForce GTX 1050\n</code></pre>"},{"location":"pytorch/tensor/#indexing","title":"Indexing","text":"<pre><code>x = torch.rand((6,5))\n# x[row,column]\nprint(x)\n</code></pre> Output <pre><code>tensor([[0.7309, 0.8946, 0.5354, 0.9422, 0.6763],\n        [0.2155, 0.2943, 0.6784, 0.7284, 0.5832],\n        [0.0695, 0.9168, 0.0840, 0.4950, 0.1248],\n        [0.1639, 0.6005, 0.7625, 0.8407, 0.8425],\n        [0.6501, 0.3108, 0.4684, 0.4940, 0.3723],\n        [0.2907, 0.1713, 0.9083, 0.2665, 0.2826]])\n</code></pre> <pre><code>x[2,4]\n</code></pre> Output <pre><code>tensor(0.1248)\n</code></pre> <pre><code>x[2] # third row\n</code></pre> Output <p><code>tensor([0.0695, 0.9168, 0.0840, 0.4950, 0.1248])</code></p> <pre><code>x[:,2] #third column\n</code></pre> Output <p><code>tensor([0.5354, 0.6784, 0.0840, 0.7625, 0.4684, 0.9083])</code></p>"},{"location":"pytorch/tensor/#concatenation","title":"Concatenation","text":"<pre><code>print('Before concatenation')\nprint(x2_,x2)\nprint('After concatenation')\nprint(torch.cat((x2_,x2),dim=0))\n</code></pre> Output <pre><code>Before concatenation\ntensor([0.1182, 0.9717, 0.3511, 0.4023], dtype=torch.float16) tensor([0.2275, 0.6938, 0.1953, 0.4771], dtype=torch.float16)\nAfter concatenation\ntensor([0.1182, 0.9717, 0.3511, 0.4023, 0.2275, 0.6938, 0.1953, 0.4771],\n    dtype=torch.float16)\n</code></pre> <pre><code>o = torch.ones(3,3)\nz = torch.zeros(3,3)\ndisplay(o,z)\nprint('After concatenation')\nprint(torch.cat((o,z),dim=0)) #  ( for this the\n# number of column of the two tensors must be same)\nprint(torch.cat((o,z),dim=1)) # for this the number of rows must be same\n</code></pre> Output <pre><code>tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n\nAfter concatenation\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.]])\n</code></pre>"},{"location":"pytorch/tensor/#multiply-two-tensors-using-matmul","title":"Multiply two tensors using matmul","text":"<pre><code>print(torch.matmul(z,o))\n</code></pre> Output <pre><code>tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n</code></pre> <pre><code>x=torch.ones(5,1,4,1)\ny=torch.ones(  3,1,1)\nprint((x+y).size())\n</code></pre> Output <p><code>torch.Size([5, 3, 4, 1])</code></p>"},{"location":"pytorch/tensor/#broadcasting-in-tensors","title":"Broadcasting in tensors","text":"<p>Well broadcasting is basically a way to get around the problem of adding dissimilar shaped tensors.  So if you are trying to add any two tensors of different shapes, then pytorch will <code>broadcast</code> the tensors in such a way that the addition of those two tensors would be possible. </p> <p>So there are a few rules which are to be followed. - Each tensor must be of at least one dimensional - lets say <code>[a,b,c,d]</code> and <code>[a,x,y]</code> are the the shapes of two tensors, then all the corresponding dimensions should be either equal or one of them has to be equal to 1 or none. </p> <p>Example :</p> <p>consider the following shapes  <pre><code>(5,1,5,7)\n# (dim0, dim1, dim2, dim3)\n(1,1,7) \n</code></pre> These two can be added because it hold the 2nd condition. </p> <p>but <code>(5,1,5,7)</code> and <code>(1,1,7,9)</code> these two can't be added because either of the <code>dim2</code> is not <code>1</code>. </p> <p>and the shape of the resulting tensor is <code>(max(x.dim0,y.dim0),max(x.dim1,y.dim1), max(x.dim2,y.dim2), max(x.dim3,y.dim3)...)</code></p> <pre><code>x=torch.ones(5,1,5,7)\ny=torch.ones(1,1,7)\nprint(x.shape,y.shape)\nprint((x+y).shape)\n</code></pre> Output <pre><code>torch.Size([5, 1, 5, 7]) torch.Size([1, 1, 7])\n\ntorch.Size([5, 1, 5, 7])\n</code></pre> <pre><code>x=torch.ones(5,1,4,1)\ny=torch.ones(  3,5,1)\nprint((x+y).size())\n</code></pre> <p>Output</p> <pre><code>---------------------------------------------------------------------------\n\nRuntimeError                              Traceback (most recent call last)\n\nCell In[52], line 3\n    1 x=torch.ones(5,1,4,1)\n    2 y=torch.ones(  3,5,1)\n----&gt; 3 print((x+y).size())\n\n\nRuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2\n</code></pre> <pre><code>x=torch.ones(5,1,4,1)\ny=torch.ones(  3,4,1)\nprint((x+y).size())\n</code></pre> Output <p>torch.Size([5, 3, 4, 1])</p>"},{"location":"pytorch/tensor1/","title":"Tensor operations","text":""},{"location":"pytorch/tensor1/#vstack-hstack","title":".vstack() &amp; .hstack()","text":"<pre><code>a = torch.randn((30,2))\nb = torch.randn((30,2))\nc = torch.randn((30,2))\nprint(a.shape,b.shape,c.shape)\nX = torch.vstack((a, b, c))\ny = torch.hstack((a,b,c))\nprint('Shape after vstack()',X.shape)#  ( for this the\n# number of column of the two tensors must be same)\nprint('Shape after hstack() ',y.shape)# for this the number of rows must be same\n</code></pre> <p>output</p> <p><code>torch.Size([30, 2]) torch.Size([30, 2]) torch.Size([30, 2])</code></p> <p><code>Shape after vstack() torch.Size([90, 2])</code></p> <p><code>Shape after hstack()  torch.Size([30, 6])</code></p>"}]}